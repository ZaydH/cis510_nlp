\documentclass[9pt]{extarticle}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi)
% \usepackage{fullpage} % changes the margin
\usepackage[margin=0.3in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{multicol}

\usepackage{subcaption}

\usepackage{enumitem}
\setitemize{itemsep=0pt,topsep=0pt}

\usepackage{soul}  % Used for \hl highlight command

\usepackage{titlesec}  % Used to adjust title heading
% Format:  \titlespacing{command}{left spacing}{before spacing}{after spacing}[right]
\titlespacing{\section}{0pt}{6pt plus 4pt minus 0pt}{4pt plus 2pt minus 0pt}
\titlespacing{\subsection}{0pt}{2pt}{2pt}
\titlespacing{\subsubsection}{0pt}{2pt}{2pt}

\setlength{\parindent}{0pt}

\newcommand{\horizontalbreak}{
  {
    \begin{center}
      \noindent\rule{7.5in}{0.4pt}
    \end{center}
  }
}

\input{global_macros}
\usepackage[dvipsnames]{xcolor}
\renewcommand{\green}[1]{{\color{ForestGreen} #1}}

\newcommand{\bluebf}[1]{\textbf{\blue{#1}}}
\newcommand{\greenbf}[1]{\textbf{\green{#1}}}
\newcommand{\redbf}[1]{\textbf{\red{#1}}}

\begin{document}
% Space above and below equation
\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

% \begin{center}
% CIS510 NLP Exam Notes Sheet -- Zayd Hammoudeh
% \end{center}
\begin{multicols}{3}
  \section*{Lecture~\#1: \hl{Introduction}}

  \textbf{NLP Applications}: Machine translation, question answering, interactive systems (SIRI), information extraction, text mining

  \textbf{Challenges of NLP}:
  \begin{itemize}
    \item Language \textit{discrete}. Harder to optimize
    \item Language \textit{compositional}. Meaning from individual words \& how combine
    \item \textit{Flexible}: Multiple ways to talk about same thing
    \item \textit{Ambiguity}: Context needed to understand word/sentence
  \end{itemize}

  \textbf{Necessary Functionality}
  \begin{itemize}
    \item \textit{Word Segmentation}: Some languages (Chinese) have no spaces between words
    \item \textit{Morphology}: Words appear different forms. Sing.\ v plural. Pres.\ v past tense
    \item \textit{Polysemous}: Sing.\ word mult.\ meaning
  \end{itemize}

  \section*{Lecture~\#2: \hl{Text Classification}}
  \textit{Example Tasks}: Topic, sentiment, language, authorship (forensics), filtering (spam)

  \greenbf{Cosine Similarity} $\frac{u \cdot v}{\norm{u}_{2} \norm{v}_{2}}$

  \subsection*{\blue{Naive Bayes}}

  Based on \textit{bag of words} and \textit{naive assumption of independence of word probabilities}. Fast w.\ low storage requirements. Robust to irrelevant features. Extendable to more feats.

  \begin{equation*}\label{eq:L02:NaiveBayes}
    \argmax_c \Pr\sbrack{c \vert X} = \argmax_{c} \frac{\Pr[X \vert c] \Pr[c]}{\Pr[X]}
  \end{equation*}

  \greenbf{Prior}: $\Pr[c] = \frac{\text{\#docs label } c \text{ in } D}{\text{\#docs in } D}$

  \greenbf{Bernoulli}: \\ $\Pr[w_ i \vert c] = \frac{\text{\# docs labeled } c \text{ containing } w_i \text{ in } D}{\text{\#docs labeled } c \text{ in } D}$

  \greenbf{Multinomial}: \\ $\Pr[w_i \vert c] = \frac{\text{\# inst.\ } w_i \text{ in doc labeled } c \text{ in } D}{\text{Tot.\ len docs labeled } c \text{ in } D}$

  \greenbf{Add-1 Smoothing}: Add $\alpha$ to numerator and $\alpha\abs{V}$ to denominator of all words

  \greenbf{IDF}: $idf_i = \log \left(N/n_i\right)$ \\ $N$ size of collection \& $n_i$ \#docs contain $w_i$

  \subsection*{\blue{Metrics}}

  \greenbf{Precision} $\frac{\text{TP}}{\text{TP} + \text{FP}}$ \hspace{0.1cm} \greenbf{Recall} $\frac{\text{TP}}{\text{TP} + \text{FN}}$

  \greenbf{F1} $\frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$

  \greenbf{Macroaveraging}: Compute performance for each class then average

  \greenbf{Microaveraging}: Collect decisions for all class, compute confusion, evaluate. \textit{Dominated by score on common classes}

  Microaveraging preferable if classes imbalanced

  \section*{Lecture~\#3: \hl{Word Embeddings}}
  \greenbf{Distributional Semantics}: Word's meaning given by words frequently appear close-by

  \textit{Context}: Set of words that appear nearby in fixed-size window

  \subsection*{\blue{Word2Vec}}

  \greenbf{Continuous Bag of Words} \\ ${\Pr\sbrack{w_t \vert w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}}}$

  \greenbf{Skip-Grams} ${\Pr\sbrack{w_{t+i} \vert w_{t}}, i \in \set{-2,-1,1,2}}$
  \begin{itemize}
    \item $v_w$: $w$ is center word
    \item $u_w$: $w$ is context word
  \end{itemize}

  \begin{equation*}
    L = \prod_{i=1}^{N} \prod_{\substack{-m \leq j \leq m \\ j \ne 0}} \Pr\sbrack{w_{i+j} \vert w_{i} ; \theta}
  \end{equation*}

  \begin{equation*}
    \Pr\sbrack{w_{i+j} \vert w_{i} ; \theta} = \frac{\exp\left(u_{w_{i+j}} \cdot v_{w_i} \right)}{\sum_{w \in V} \exp\left(u_{w} \cdot v_{w_i} \right)}
  \end{equation*}

  \greenbf{Negative Sampling}: Sample words u.a.r.\ \& and use to estimate denominator

  \subsection*{\blue{Co-occurrence Counts}}

  Create $A$ is ${\abs{V} \times \abs{V}}$ matrix of where $a_{i,j}$ is \#docs words $w_i$ \& $w_j$ appear together

  \greenbf{Method~\#1}: Dimensionality red.\ (SVD)

  \greenbf{Method~\#2}: GloVe
  \begin{itemize}
    \item Ratio of co-occurrences can encode relationships between words
  \end{itemize}

  \subsection*{Evaluation of Embeddings}

  \greenbf{Intrinsic}: Eval.\ on specific/intermediate task
  \begin{itemize}
    \item Fast to compute
    \item Helps to understand system
    \item Unclear if really helpful unless correlates real task
  \end{itemize}

  \greenbf{Extrinsic}: Evaluate on real task e.g.,~NER
  \begin{itemize}
    \item Long time to evaluate
  \end{itemize}

  \section*{Lecture~\#4: \hl{Deep Learning}}
  \bluebf{Logistic Regression} (MaxEnt):
  \begin{equation*}\label{eq:L04:LogReg}
    \mathcal{L}(\mathbf{w},b) = -\sum_{\mathbf{x}_i,y_i \in \mathbf{d}} \log \sigma\big( y_i(\mathbf{w}\cdot \mathbf{x}_i + b) \big)
  \end{equation*}

  \greenbf{Cross Entropy} $H(p,q) = \mathbb{E}_{p(x)} \sbrack{-\log q}$

  \subsection*{Activation Functions}

  $\sigma(a) = \frac{1}{1+exp(-a)}$

  $\text{SoftPlus}(a) = \log(1 + e{a})$

  $\text{ELU}(a) = \begin{cases}
                      a & a \geq 0 \\
                      \alpha (e^{a} - 1) & a < 0
                   \end{cases}$

  \greenbf{Chain Rule}: $(f \circ g)' = (f' \cdot g)g'$

  \section*{Lecture~\#5: \hl{Sequential Labeling}}
  \subsection*{\blue{Hidden Markov Model}}

  \begin{align*}
    \argmax_{\vec{y}} \Pr\sbrack{\vec{y} \vert \vec{x}} &= \argmax_{\vec{y}} \frac{\Pr\sbrack{\vec{x} \vert \vec{y}}\Pr[\vec{y}]}{\Pr[\vec{x}]} \\
                                                        &= \argmax_{\vec{y}} \Pr\sbrack{\vec{x} \vert \vec{y}} \Pr[\vec{y}]
  \end{align*}

  \greenbf{First-Order Markov Assumption}:

  \begin{equation*}
    \Pr\sbrack{y_{1},\ldots,y_{n}} = \prod_{i=1}^{n} \Pr[y_{i} \vert y_{i-1}]
  \end{equation*}

  \greenbf{Indepen.\ Assum} $\Pr\sbrack{\vec{x} \vert \vec{y}} = \prod_{i=1}^{n} \Pr[x_{i} \vert y_{i}]$

  \greenbf{Transition Prob}: $\Pr\sbrack{y_{t} \vert y_{t-1}}$

  \greenbf{Emission Prob}: $\Pr\sbrack{x_{t} \vert y_{t}}$

  \subsubsection*{\red{Decoding}}

  \greenbf{Greedy Decoding}: \\${y_t = \argmax_{y'} \Pr[x_t \vert y'] \Pr[y' \vert y_{t-1}]}$

  \greenbf{Viterbi}
  \begin{itemize}
    \item \textit{Complexity}: $O(\abs{S}^{2} n)$
    \item \textit{Initialization}: $v_0(s) = \begin{cases}
                                               1 & s = \text{start} \\
                                               0 & \text{Otherwise}
                                             \end{cases}$
    \item \textit{Termination}: $t = n+1$
  \end{itemize}

  \begin{equation*}
    v_{t}(s) = \max_{s' \in Y}\set{v_{t-1}(s') \Pr[s \vert s'] \Pr[x_t \vert s]}
  \end{equation*}

  \subsection*{\blue{Max.\ Entropy Markov Model}}
  \subsection*{\blue{Conditional Random Field}}
  \begin{itemize}
    \item Complexity: $O(n\abs{Y}^{2})$ where $Y$ set of label vectors
  \end{itemize}

  \begin{equation*}
    \Pr[y \vert x; \theta] = \frac{\exp(\Phi(\vec{x},\vec{y})\transpose\theta))}{\sum_{\vec{y}' \in Y} \exp(\Phi(\vec{x}, \vec{y'})\transpose\theta)}
  \end{equation*}

  where:
  \begin{align*}
      \Phi(\vec{x},\vec{y}) &= \sbrack{\Phi_{1}(\vec{x},\vec{y}),\ldots,\Phi_{K}(\vec{x},\vec{y})} \\
      \Phi_{k}(\vec{x},\vec{y}) &= \sum_{i=1}^{n} \phi_{k}(y_{i-1},y_{i},\vec{x},i)
  \end{align*}
  \subsection*{\blue{Bidirectional RNN}}

  \begin{itemize}
    \item Can incorporate CRF as final layer in sequence labeling
  \end{itemize}

  \section*{Lecture~\#6: \hl{Constituent Parsing}}

  \greenbf{Constituency}: Group of words that behave as single unit

  \greenbf{Substitution Test}: If a constituent is replaced by another constituent of same type, does sentence remain \textit{grammatical}

  \greenbf{Context-Free Grammar} Formal def.\ meaningful constituents \& how constituent is formed from other constituents.  \textit{Every internal tree node is a phrase that can be replaced by another of same type of constituent}
  \begin{itemize}
    \item $N$: Set of non-terminal symbols
    \item $\Sigma$: Alphabet terminal symbols
    \item $R$: Set of production rules $A \rightarrow \beta$
    \item $S$: Start symbol
  \end{itemize}

  \textbf{Language}: All strings derivable from $S$

  \textbf{Preterminals}: Set symbols can be rewritten as terminals

  \textbf{Syntax}: Move from labeling discrete items to structure between items

  \greenbf{Attachment Ambiguity}: A particular constituent can be attached to the parse tree at more than one place
  \begin{itemize}
    \item ``I saw the man with a telescope''
  \end{itemize}

  \greenbf{Coordination Ambiguity} Diff sets phrases can be conjoined by conjunction like ``and''
  \begin{itemize}
    \item ``Old man and woman''
  \end{itemize}

  \greenbf{Treebank}: Collection of sentences annotated with syntactic structure

  \greenbf{Chomsky Normal Form} (CNF): All rules have form ${A \rightarrow \beta}$ where $\beta$~either single terminal in~$\Sigma$ or two non-terminals in~$N$

  \subsection*{\blue{CYK Parsing}}

  \begin{itemize}
    \item \red{Left-to-right, bottom-up}
    \item \red{Every time has below it a range ${[start,end)}$}
    \item Checks whether sentence grammatical in CFG's language
    \item Enumerate all poss.\ parses for sentence
  \end{itemize}

  \greenbf{Probabilistic Context-Free Grammar}: Tells which parse of sentence more likely
  \begin{itemize}
    \item Each production associated w.\ prob.
    \item \textbf{Strong Independence Assumption} where ${\Pr(T) = \prod_{i=1}^{n} \Pr(\beta \vert A)}$. In practice, productions strongly dependent on place in tree.
  \end{itemize}

  \begin{equation*}
    \Pr(\beta \vert A) = \frac{\text{Count}(A \rightarrow \beta)}{\text{Count}(A)}
  \end{equation*}

  \subsection*{\blue{Evaluation}}

  Tuple ${\langle l_k, i_k, j_k  \rangle}$ where $l_k$ label for $k$\-/th phrase, $i_k$ start index $k$\-/th phrase, $j_k$ end index $k$\-/th phrase
  \begin{itemize}
    \item Tuple correct if all elements in tuple match correct tree
  \end{itemize}

  \section*{Lecture~\#7: \hl{Dependency Parsing}}

  \greenbf{Dependency Structure}: Directed graph consisting of set of vertices $V$ and arcs~$A$

  \textit{Approaches}: DP, graph alg, CSP, transition-based parsing

  \begin{itemize}
    \item Closer to semantic relations than constituents
    \item Except root, exactly one incoming edge. Tail is dependent on head word
    \item Not tied to linear order of words making more language syntax independent. \textit{Free of word order}
    \item \textbf{DP Complexity}: $O(n^3)$ (Eisner's)
  \end{itemize}

  \subsection*{Greedy Dependency Parsing}

  \greenbf{Guide}: Predicts next transition given current config

  \greenbf{Arc-Standard Algorithm}

  No guarantee finds the best tree.

  \begin{itemize}
    \item \textbf{Complexity}: $O(n)$
    \item \textbf{Configuration}: Buffer, stack \& dependency tree
    \item \textbf{Init Config}: Stack \& tree empty. All words in buffer
    \item \textbf{Term Config}: Buffer empty. Stack is single word
    \item \textbf{\#Classes}: 1 + 2 * arc type
  \end{itemize}

  \textbf{Operations}
  \begin{itemize}
    \item \texttt{shift}: Move from buffer to top of stack
    \item \texttt{left-arc}: Add arc from top of stack to penultimate on stack. Pop penultimate.
    \item \texttt{right-arc}: Add arc from penultimate of stack to top. Pop top.
  \end{itemize}

  \redbf{Problem}: Feature calculation slow so slow use DNN.

  Arcs also have a type.

  \subsection*{\blue{Evaluation}}

  \greenbf{Labeled Attachment Score} (LAS): Percentage of correct arcs relative to gold

  \greenbf{Labeled Exact Match} (LEM): Percentage of correct dependency trees

  \greenbf{Unlabeled Attachment Score/Exact Match} (UAS/UEM): Same as above but ignore arc labels

  \section*{Lecture~\#8: \hl{Information \& Relation Extraction}}

  \greenbf{Information Extraction}: Automatic extraction of structured information from unstructured or semi-structure docs
  \begin{itemize}
    \item Returns \textit{facts} from documents
  \end{itemize}

  \greenbf{Information Retrieval}: Returns a set of documents given a query

  \greenbf{Relation Extraction}: Task extract semantic relationships from text
  \begin{itemize}
    \item \redbf{Relation}: Predication about pair
  \end{itemize}

  \greenbf{Co-Reference Resolution}: Find all expressions that refer to same entity in text

  \greenbf{Entity Linking}: Task recognize \& disambiguate named entities to a knowledge base

  \redbf{Trigger Prediction}:

  \redbf{Argument Prediction}:

  \greenbf{Gazetteer}: List of common names for different types

  \subsection*{\blue{Evaluation}}

  \greenbf{NER}: Tuple ${\langle i_k, j_k, l_k \rangle}$ for start, end, \& type of entity

  \subsection*{\blue{Semi-Supervised Learning}}

  \textbf{Termination Conditions}
  \begin{itemize}
    \item $U$ exhausted
    \item Performance goal reached
    \item Fixed number of iterations
  \end{itemize}

  \redbf{Problem}: Errors in early greedy confidence choices rapidly magnified

  \subsection*{\blue{Co-Training}}
  \begin{itemize}
    \item Two \textit{views} of data (subset features)
    \item Alternate training separate classifiers on feature subsets
  \end{itemize}

\end{multicols}


\begin{figure}[]
  \centering
  \begin{minipage}{.47\textwidth}
    \begin{algorithm}[H]
      \caption{CYK algorithm}
      \begin{algorithmic}[1]
        \For{$j \gets 1 \mathbf{to} \textproc{Length}(words)$}
          \ForEach{$\set{A \vert A \rightarrow words[j] \in grammar}$}
            \State $table[j-1,j] \gets table[j-1,j] \cup A$
          \EndFor
          \For{$i \gets j-2 \text{ down to } 0$}
            \For{$k \gets i+1 \text{ to } j - 1$}
              \ForEach{$\set{A \vert A \rightarrow BC \in grammar \wedge B \in table[i,k] \wedge C \in table[k,j]}$}
                \State $table[i,j] \gets table[i,j] \cup A$
              \EndFor
            \EndFor
          \EndFor
        \EndFor
      \end{algorithmic}
    \end{algorithm}
%
%     for $j \gets 1 \text{ to } Length(Sentence)$ \\
%     \hspace*{0.2cm}for $i \gets j-2 \text{ down to } 0$ \\
%     \hspace*{0.4cm}for $k \gets i+1 \text{ to } j - 1$ \\
    % \hspace*{0.6cm}for $\set{A \vert A \rightarrow BC \in G \wedge B \in table[i,k] \wedge C \in table[k,j]}$
%     \hspace*{0.8cm}$table[i,j] \gets table[i,j] \cup A$
  \end{minipage}%
  \begin{minipage}{.47\textwidth}
    \begin{algorithm}[H]
      \caption{Probabilistic CYK algorithm}
      \begin{algorithmic}[1]
        \For{$j \gets 1 \mathbf{to} \textproc{Length}(words)$}
          \ForEach{$\set{A \vert A \rightarrow words[j] \in grammar}$}
          \State $table[j-1,j, A] \gets \Pr(A \rightarrow words[j])$
          \EndFor
          \For{$i \gets j-2 \text{ down to } 0$}
            \For{$k \gets i+1 \text{ to } j - 1$}
              \ForEach{$\set{A \vert A \rightarrow BC \in grammar \wedge table[i,k, B] > 0 \wedge table[k,j,C] > 0}$}
                \State $table[i,j] \gets table[i,j] \cup A$
              \EndFor
            \EndFor
          \EndFor
        \EndFor
      \end{algorithmic}
    \end{algorithm}
  \end{minipage}
\end{figure}

\begin{table}[b]
  \centering
  % \caption{}\label{tab:}
  \begin{tabular}{|c|l|c|l|}
    \hline
    \bluebf{PRP} Personal Pronoun & \multicolumn{3}{l|}{\textit{I}, \textit{me}, \textit{you}, \textit{he}, \textit{him}, \textit{it}, Reflective pronoun end in \texttt{-self} (\textit{himself}) nominal possessive pronoun (\textit{mine}, \textit{yours}, \textit{hers})} \\\hline
    \bluebf{DT} Determiner & \multicolumn{3}{l|}{Articles (\textit{a}, \textit{the}, \textit{every}, \textit{no}), \textit{that}, \textit{these}, \textit{this}, \textit{those} precede noun. \textit{All}, \textit{both} not precede \texttt{DT} or \texttt{PRP\$}}  \\\hline
    \bluebf{PDT} Predeterminer & Precede article or possessive pronoun & \bluebf{VB} Base Form & In imperative, infinites, subjectives \textit{do}\\\hline
    \bluebf{CC} Coord.\ Conjunction & \multicolumn{3}{l|}{\textit{And}, \textit{but}, \textit{not}, \textit{or}, math operators (\textit{plus}, \textit{minus}, \textit{less}, \textit{times}), \textit{For} meaning because} \\\hline
    \bluebf{RB} Adverb & \multicolumn{3}{l|}{Most words end in \texttt{-ly}, degree words (\textit{quite}, \textit{too}, \textit{very}), negative markers (\textit{not}, \textit{n't}, \textit{never})} \\\hline
    \bluebf{VBP} Present & Non-3rd.\ I \textit{am}, You \textit{are} & \bluebf{VBZ} Third Person & She \textit{is}. He \textit{likes} \\\hline
    \bluebf{VBG} \green{Present} Participle & Verb functions as noun ending \textit{-ing} & \bluebf{VBN} \red{Past} Participle & ends in \texttt{-en} or \texttt{-ed} \\\hline
    \bluebf{RBR} Comparative Adv. & Adverb end in \texttt{-er}. \textit{More}, \textit{Less} & \bluebf{RBS} Superlative Adv. & Adverb end in \texttt{-est}. \textit{Most}, \textit{Least} \\\hline
    \bluebf{RP} Particle & \textit{over}, \textit{down}, \textit{out}, \textit{on} & \bluebf{JJ} Adjective & General adj., ordinal number (\textit{fourth}) \\\hline
    \bluebf{JJR} Comparative Adj. & Adjective end in \texttt{-er}. \textit{More}, \textit{less} & \bluebf{JJS} Superlative Adj. & Adjective end in \texttt{-est}. \textit{Most}, \textit{least} \\\hline
    \bluebf{PRP\$} Possess.\ Pronoun & \textit{my}, \textit{their}, \textit{its}, \textit{his}, \textit{her} & \bluebf{IN} Preposition & All preposition except ``to''. \textit{On}, \textit{because}, \textit{from} \\\hline
  \end{tabular}
\end{table}

\end{document}
