\documentclass[9pt]{extarticle}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi)
% \usepackage{fullpage} % changes the margin
\usepackage[margin=0.3in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{multicol}

\usepackage{subcaption}

\usepackage{titlesec}  % Used to adjust title heading
% Format:  \titlespacing{command}{left spacing}{before spacing}{after spacing}[right]
\titlespacing\section{0pt}{6pt plus 4pt minus 0pt}{4pt plus 2pt minus 0pt}

\setlength\parindent{0pt}
\setlength\itemsep{0pt}

\newcommand{\horizontalbreak}{
  {
    \begin{center}
      \noindent\rule{7.5in}{0.4pt}
    \end{center}
  }
}

\input{global_macros}
\usepackage[dvipsnames]{xcolor}
\renewcommand{\green}[1]{{\color{ForestGreen} #1}}

\begin{document}
\begin{center}
CIS510 NLP Exam Notes Sheet -- Zayd Hammoudeh
\end{center}
\begin{multicols}{3}
  \section*{Lecture~\#1: Introduction}

  \textbf{NLP Applications}: Machine translation, question answering, interactive systems (SIRI), information extraction, text mining

  \textbf{Challenges of NLP}:
  \begin{itemize}
    \item Language \textit{discrete}. Harder to optimize
    \item Language \textit{compositional}. Meaning comes from individual words \& how they combine
    \item \textit{Flexible}: Multiple ways to talk about same thing
    \item \textit{Ambiguity}: Context needed to understand word/sentence
  \end{itemize}

  \textbf{Necessary Functionality}
  \begin{itemize}
    \item \textit{Word Segmentation}: Some languages (e.g.,~Chinese) have no spaces between words
    \item \textit{Morphology}: Words appear different forms. Sing.\ v plural. Pres.\ v past tense
    \item \textit{Polysemous}: Single word has multiple meanings
  \end{itemize}

  \section*{Lecture~\#2: Text Classification}
  \textit{Example Tasks}: Topic, sentiment, language, authorship (forensics), filtering (spam)

  \textbf{\green{Cosine Similarity}} $\frac{u \cdot v}{\norm{u}_{2} \norm{v}_{2}}$

  \subsection*{\blue{Naive Bayes}}

  Based on \textit{bag of words} and \textit{naive assumption of independence of word probabilities}. Fast w.\ low storage requirements. Robust to irrelevant features. Extendable to more feats.

  \begin{equation*}\label{eq:L02:NaiveBayes}
    \argmax_c \Pr\sbrack{c \vert X} = \argmax_{c} \frac{\Pr[X \vert c] \Pr[c]}{\Pr[X]}
  \end{equation*}

  \green{\textbf{Prior}}: $\Pr[c] = \frac{\text{\#docs label } c \text{ in } D}{\text{\#docs in } D}$

  \green{\textbf{Bernoulli}}: \\ $\Pr[w_ i \vert c] = \frac{\text{\# docs labeled } c \text{ containing } w_i \text{ in } D}{\text{\#docs labeled } c \text{ in } D}$

  \green{\textbf{Multinomial}}: \\ $\Pr[w_i \vert c] = \frac{\text{\# inst.\ } w_i \text{ in doc labeled } c \text{ in } D}{\text{Tot.\ len docs labeled } c \text{ in } D}$

  \green{\textbf{Add-1 Smoothing}}: Add $\alpha$ to numerator and $\alpha\abs{V}$ to denominator of all words

  \textbf{\green{IDF}}: $idf_i = \log \left(N/n_i\right)$ \\ $N$ size of collection \& $n_i$ \#docs contain $w_i$

  \subsection*{\blue{Metrics}}

  \textbf{\green{Precision}} $\frac{\text{TP}}{\text{TP} + \text{FP}}$ \hspace{0.1cm} \textbf{\green{Recall}} $\frac{\text{TP}}{\text{TP} + \text{FN}}$

  \textbf{\green{F1}} $\frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$

  \section*{Lecture~\#3: Word Embeddings}
  \textbf{\green{Distributional Semantics}}: Word's meaning given by words frequently appear close-by

  \textit{Context}: Set of words that appear nearby in fixed-size window

  \subsection*{\blue{Word2Vec}}

  \textbf{\green{Continuous Bag of Words}} \\ ${\Pr\sbrack{w_t \vert w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}}}$

  \textbf{\green{Skip-Grams}} ${\Pr\sbrack{w_{t+i} \vert w_{t}}, i \in \set{-2,-1,1,2}}$
  \begin{itemize}
    \item $v_w$: $w$ is center word
    \item $u_w$: $w$ is context word
  \end{itemize}

  \begin{equation*}
    L = \prod_{i=1}^{N} \prod_{\substack{-m \leq j \leq m \\ j \ne 0}} \Pr\sbrack{w_{i+j} \vert w_{i} ; \theta}
  \end{equation*}

  \begin{equation*}
    \Pr\sbrack{w_{i+j} \vert w_{i} ; \theta} = \frac{\exp\left(u_{w_{i+j}} \cdot v_{w_i} \right)}{\sum_{w \in V} \exp\left(u_{w} \cdot v_{w_i} \right)}
  \end{equation*}

  \green{\textbf{Negative Sampling}}: Sample words u.a.r.\ \& and use to estimate denominator

  \subsection*{\blue{Co-occurrence Counts}}

  Create $A$ is ${\abs{V} \times \abs{V}}$ matrix of where $a_{i,j}$ is \#docs words $w_i$ \& $w_j$ appear together

  \green{\textbf{Method~\#1}}: Dimensionality reduction (e.g.,~SVD)

  \green{\textbf{Method~\#2}}: GloVe
  \begin{itemize}
    \item Ratio of co-occurrences can encode relationships between words
  \end{itemize}

  \subsection*{Evaluation of Embeddings}

  \textbf{\green{Intrinsic}}: Eval.\ on specific/intermediate task
  \begin{itemize}
    \item Fast to compute
    \item Helps to understand system
    \item Unclear if really helpful unless correlates real task
  \end{itemize}

  \textbf{\green{Extrinsic}}: Evaluate on real task e.g.,~NER
  \begin{itemize}
    \item Long time to evaluate
  \end{itemize}

  \section*{Lecture~\#4: Deep Learning}
  \textbf{\blue{Logistic Regression}} (MaxEnt):
  \begin{equation*}\label{eq:L04:LogReg}
    \mathcal{L}(\mathbf{w},b) = -\sum_{\mathbf{x}_i,y_i \in \mathbf{d}} \log \sigma\big( y_i(\mathbf{w}\cdot \mathbf{x}_i + b) \big)
  \end{equation*}

  \textbf{\green{Cross Entropy}} $H(p,q) = \mathbb{E}_{p(x)} \sbrack{-\log q}$

  \subsection*{Activation Functions}

  $\sigma(a) = \frac{1}{1+exp(-a)}$

  $\text{SoftPlus}(a) = \log(1 + e{a})$

  $\text{ELU}(a) = \begin{cases}
                      a & a \geq 0 \\
                      \alpha (e^{a} - 1) & a < 0
                   \end{cases}$

  \textbf{\green{Chain Rule}}: $(f \circ g)' = (f' \cdot g)g'$

  \section*{Lecture~\#5: Sequential Labeling}
  \subsection*{\blue{Hidden Markov Model}}

  \begin{align*}
    \argmax_{\vec{y}} \Pr\sbrack{\vec{y} \vert \vec{x}} &= \argmax_{\vec{y}} \frac{\Pr\sbrack{\vec{x} \vert \vec{y}}\Pr[\vec{y}]}{\Pr[\vec{x}]} \\
                                                        &= \argmax_{\vec{y}} \Pr\sbrack{\vec{x} \vert \vec{y}} \Pr[\vec{y}]
  \end{align*}

  \textbf{\green{First-Order Markov Assumption}}:

  \begin{equation*}
    \Pr\sbrack{y_{1},\ldots,y_{n}} = \prod_{i=1}^{n} \Pr[y_{i} \vert y_{i-1}]
  \end{equation*}

  \textbf{\green{Indepen.\ Assum}} $\Pr\sbrack{\vec{x} \vert \vec{y}} = \prod_{i=1}^{n} \Pr[x_{i} \vert y_{i}]$

  \textbf{\green{Transition Prob}}: $\Pr\sbrack{y_{t} \vert y_{t-1}}$

  \textbf{\green{Emission Prob}}: $\Pr\sbrack{x_{t} \vert y_{t}}$

  \subsubsection*{\red{Decoding}}

  \textbf{\green{Greedy Decoding}}: \\${y_t = \argmax_{y'} \Pr[x_t \vert y'] \Pr[y' \vert y_{t-1}]}$

  \textbf{\green{Viterbi}}
  \begin{itemize}
    \item \textit{Complexity}: $O(\abs{S}^{2} n)$
    \item \textit{Initialization}: $v_0(s) = \begin{cases}
                                               1 & s = \text{start} \\
                                               0 & \text{Otherwise}
                                             \end{cases}$
    \item \textit{Termination}: $t = n+1$
  \end{itemize}

  \begin{equation*}
    v_{t}(s) = \max_{s' \in Y}\set{v_{t-1}(s') \Pr[s \vert s'] \Pr[x_t \vert s]}
  \end{equation*}

  \subsection*{Max.\ Entropy Markov Model}
  \subsection*{\blue{Conditional Random Field}}

  \begin{equation*}
    \Pr[y\vert x ; \theta] = \frac{\exp(\Phi(\vec{x},\vec{y})\transpose\theta))}{\sum_{\vec{y}' \in Y} \exp(\Phi(\vec{x}, \vec{y'})\transpose\theta)}
  \end{equation*}

  where:

  \begin{align*}
      \Phi(\vec{x},\vec{y}) &= \sbrack{\Phi_{1}(\vec{x},\vec{y}),\ldots,\Phi_{K}(\vec{x},\vec{y})} \\
      \Phi_{k}(\vec{x},\vec{y}) &= \sum_{i=1}^{n} \phi_{k}(y_{i-1},y_{i},\vec{x},i)
  \end{align*}

  \subsection*{Bidirectional RNN}

  \begin{itemize}
    \item Can incorporate CRF as final layer in sequence labeling
  \end{itemize}

  \section*{Lecture~\#6: Constituent Parsing}

  \textbf{\green{Constituency}}: Group of words that behave as single unit

  \textbf{\green{Context-Free Grammar}}: Formal way to define meaningful constituents and how a constituent is formed from other constituents
  \begin{itemize}
    \item $N$: Set of non-terminal symbols
    \item $\Sigma$: Alphabet terminal symbols
  \end{itemize}

  \textbf{Language}: All strings derivable from start symbol

  \textbf{Preterminals}: Set symbols can be rewritten as terminals

  \textbf{Syntax}: Move from label discrete items to structure between items

  \subsection*{CYK Parsing}

  \begin{itemize}
    \item Checks whether sentence grammatical in CFG's language
    \item Enumerate all possible parses for sentence
  \end{itemize}

  for $j \gets 1 \text{ to } Length(Sentence)$ \\
  \hspace*{0.2cm}for $i \gets j-2 \text{ down to } 0$ \\
  \hspace*{0.4cm}for $k \gets i+1 \text{ to } j - 1$ \\
  \hspace*{0.6cm}for $\set{A \vert A \rightarrow BC \in G \wedge B \in table[i,k] \wedge C \in table[k,j]}$
  \hspace*{0.8cm}$table[i,j] \gets table[i,j] \cup A$

  \textbf{\green{Probabilistic Context-Free Grammar}}: Tells which parse of sentence more likely

  \section*{Lecture~\#7: Dependency Parsing}

  \textbf{\green{Dependency Structure}}: Directed graph consisting of set of vertices $V$ and arcs~$A$
  \begin{itemize}
    \item Except root, exactly one incoming edge
    \item Not tied to linear order of words making more language syntax independent
  \end{itemize}

  \section*{Lecture~\#8: Information \& Relation Extraction}

\end{multicols}


\end{document}
