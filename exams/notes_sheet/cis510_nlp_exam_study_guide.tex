\documentclass[8pt]{extarticle}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi)
% \usepackage{fullpage} % changes the margin
\usepackage[margin=0.3in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{multicol}

\usepackage{subcaption}

\usepackage{enumitem}
\setitemize{itemsep=-2pt,topsep=0pt}

\usepackage{soul}  % Used for \hl highlight command

\usepackage{titlesec}  % Used to adjust title heading
% Format:  \titlespacing{command}{left spacing}{before spacing}{after spacing}[right]
\titlespacing{\section}{0pt}{2pt}{2pt}
\titlespacing{\subsection}{0pt}{2pt}{2pt}
\titlespacing{\subsubsection}{0pt}{2pt}{2pt}

\setlength{\parindent}{0pt}
% Remove space after table
\setlength{\floatsep}{0cm}
\setlength{\textfloatsep}{0cm}

\newcommand{\horizontalbreak}{
  {
    \begin{center}
      \noindent\rule{7.5in}{0.4pt}
    \end{center}
  }
}

\input{global_macros}
\usepackage[dvipsnames]{xcolor}
\renewcommand{\green}[1]{{\color{ForestGreen} #1}}

\newcommand{\bluebf}[1]{\textbf{\blue{#1}}}
\newcommand{\greenbf}[1]{\textbf{\green{#1}}}
\newcommand{\redbf}[1]{\textbf{\red{#1}}}

\begin{document}
% Space above and below equation
\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

% \begin{center}
% CIS510 NLP Exam Notes Sheet -- Zayd Hammoudeh
% \end{center}
\begin{multicols}{3}
  \section*{Lecture~\#1: \hl{Introduction}}

  \textbf{NLP Applications}: Machine translation, question answering, interactive systems (SIRI), information extraction, text mining

  \textbf{Challenges of NLP}:
  \begin{itemize}
    \item Language \textit{discrete}. Harder to optimize
    \item Language \textit{compositional}. Meaning from individual words \& how combine
    \item \textit{Flexible}: Mult way say same thing
    \item \textit{Ambiguity}: Context needed to understand word/sentence
  \end{itemize}

  \textbf{Necessary Functionality}
  \begin{itemize}
    \item \textit{Word Segmentation}: Some languages (Chinese) have no spaces between words
    \item \textit{Morphology}: Words appear different forms. Sing.\ v plural. Pres.\ v past tense
    \item \textit{Polysemous}: Sing.\ word mult.\ meaning
  \end{itemize}

  Words $\rightarrow$ Morphology $\rightarrow$ Syntax $\rightarrow$ Semantics $\rightarrow$ Discourse (how interpret in context/what doc struc?)

  \section*{Lecture~\#2: \hl{Text Classification}}
  \textit{Example Tasks}: Topic, knowledge organization, sentiment, language, authorship (forensics), filtering (spam)

  \greenbf{Cosine Similarity} $\frac{u \cdot v}{\norm{u}_{2} \norm{v}_{2}}$

  \textbf{Benefits of ML}: Easily adapt to changing conditions. Labeling training examples cheaper than writing classification rules.

  \subsection*{\blue{Naive Bayes}}

  Based on \textit{bag of words} and \textit{naive assumption of independence of word probabilities}. Fast w.\ low storage requirements. Robust to irrelevant features. Extendable to more feats than just words.

  \begin{equation*}\label{eq:L02:NaiveBayes}
    \argmax_c \Pr\sbrack{c \vert X} = \argmax_{c} \frac{\Pr[X \vert c] \Pr[c]}{\Pr[X]}
  \end{equation*}

  \greenbf{Prior}: $\Pr[c] = \frac{\text{\#docs label } c \text{ in } D}{\text{\#docs in } D}$

  \greenbf{Bernoulli}: \\ $\Pr[w_ i \vert c] = \frac{\text{\# docs labeled } c \text{ containing } w_i \text{ in } D}{\text{\#docs labeled } c \text{ in } D}$

  \greenbf{Multinomial}: Better long docs \\ $\Pr[w_i \vert c] = \frac{\text{\# inst.\ } w_i \text{ in doc labeled } c \text{ in } D}{\text{Tot.\ len docs labeled } c \text{ in } D}$

  \greenbf{Add\-/1 Smoothing}: Add $\alpha$ to numerator and $\alpha\abs{V}$ to denominator of all words

  \greenbf{IDF}: $idf_i = \log \left(N/n_i\right)$ \\ $N$ size of collection \& $n_i$ \#docs contain $w_i$. Weights ``unusual'' words higher.

  \redbf{Problem}: Word can be positive or negative based on context. Bag of words loses context dependence
  \textbf{Solution}: Negated vocabulary. ``never failed'' $\rightarrow$ ``NOT\_failed''

  \subsection*{\blue{Metrics}}

  \greenbf{Precision} $\frac{\text{TP}}{\text{TP} + \text{FP}}$ \hspace{0.1cm} \greenbf{Recall} $\frac{\text{TP}}{\text{TP} + \text{FN}}$

  \greenbf{F1} $\frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$

  \greenbf{Macroaveraging}: Compute performance for each class then average

  \greenbf{Microaveraging}: Collect decisions for all class, compute confusion, evaluate. \textit{Dominated by score on common classes}

  Microaveraging prefer class imbalanced

  \section*{Lecture~\#3: \hl{Word Embeddings}}
  \textbf{Thesaurus like WordNet}
  \begin{itemize}
    \item Missing new words. Challenging to keep up to date
    \item Subjective and require human labor
    \item Impossible to compute word similarity
  \end{itemize}

  \greenbf{Distributional Semantics}: Word's meaning given by words frequently appear close-by

  \greenbf{One-Hot}: Vectors large, sparse, \& orthogonal. \redbf{Localist representation}

  \greenbf{Distributed}/\greenbf{Dense Representation}: Single neuron represent many concepts.

  \textit{Context}: Set of words that appear nearby in fixed-size window

  \subsection*{\blue{Word2Vec}}

  \greenbf{Continuous Bag of Words} \\ ${\Pr\sbrack{w_t \vert w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}}}$

  \greenbf{Skip-Grams} ${\Pr\sbrack{w_{t+i} \vert w_{t}}, i \in \set{-2,-1,1,2}}$
  \begin{itemize}
    \item $v_w$: $w$ is center word
    \item $u_w$: $w$ is context word
    \item Two vectors makes optimization easier. Average $u_w$ \& $v_w$ for final word vectors.
  \end{itemize}
  \begin{equation*}
    \text{Likelihood} = \prod_{i=1}^{N} \prod_{\substack{-m \leq j \leq m \\ j \ne 0}} \Pr\sbrack{w_{i+j} \vert w_{i} ; \theta}
  \end{equation*}
  \begin{equation*}
    \text{loss} = J(\theta) = -\frac{1}{N}\sum_{i=1}^{N} \sum_{\substack{-m \leq j \leq m \\ j \ne 0}} \log \Pr\sbrack{w_{i+j} \vert w_{i} ; \theta}
  \end{equation*}
  \begin{equation*}
    \Pr\sbrack{w_{i+j} \vert w_{i} ; \theta} = \frac{\exp\left(u_{w_{i+j}} \cdot v_{w_i} \right)}{\sum_{w \in V} \exp\left(u_{w} \cdot v_{w_i} \right)}
  \end{equation*}

  \greenbf{Negative Sampling}: Sample words u.a.r.\ \& and use to estimate denominator

  \subsection*{\blue{Co-occurrence Counts}}

  Create $A$ is ${\abs{V} \times \abs{V}}$ matrix of where $a_{i,j}$ is \#docs words $w_i$ \& $w_j$ appear together

  \greenbf{Method~\#1}: Dimensionality red.\ (SVD)

  \begin{itemize}
    \item Fast to compute vs.\ of scale with corpus size
    \item Efficient usage statistics vs.\ inefficient
    \item Primarily used to capture word similarity vs.\ improved performance on other tasks
    \item Disproportionate influence given to large counts vs.\ capture complex patterns beyond word similarity
  \end{itemize}

  \greenbf{Method~\#2}: GloVe
  \begin{itemize}
    \item \redbf{Crucial Insight}: Ratio of co-occurrences can encode relationships between words
  \end{itemize}
  \begin{equation*}
    J = \sum_{i,j=1}^{\abs{V}} f(X_{ij})(w_{i}\transpose\tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^{2}
  \end{equation*}
  for weight function~$f$ and $X_{ij}$ number times word~$j$ occur in context of word~$i$
  \subsection*{Evaluation of Embeddings}

  \greenbf{Intrinsic}: Eval.\ on specific/intermediate task
  \begin{itemize}
    \item Fast to compute
    \item Helps to understand system
    \item Unclear if really helpful unless correlates real task
  \end{itemize}

  \greenbf{Extrinsic}: Evaluate on real task e.g.,~NER
  \begin{itemize}
    \item Long time to evaluate
  \end{itemize}

  \section*{Lecture~\#4: \hl{Deep Learning}}
  \bluebf{Logistic Regression} (MaxEnt):
  \begin{equation*}\label{eq:L04:LogReg}
    \mathcal{L}(\mathbf{w},b) = -\sum_{\mathbf{x}_i,y_i \in \mathbf{d}} \log \sigma\big( y_i(\mathbf{w}\cdot \mathbf{x}_i + b) \big)
  \end{equation*}

  \greenbf{Cross Entropy} $H(p,q) = \mathbb{E}_{p(x)} \sbrack{-\log q}$

  \subsection*{Activation Functions}

  $\sigma(a) = \frac{1}{1+\exp(-a)}$

  $\text{SoftPlus}(a) = \log(1 + e^{a})$

  $\text{ELU}(a) = \begin{cases}
                      a & a \geq 0 \\
                      \alpha (e^{a} - 1) & a < 0
                   \end{cases}$

  \greenbf{Chain Rule}: $(f \circ g)' = (f' \circ g)g'$

  \begin{equation*}
    \theta^{\text{new}} \gets \theta^{\text{old}} - \alpha \nabla_{\theta}J(\theta)
  \end{equation*}

  \section*{Lecture~\#5: \hl{Sequential Labeling}}

  \textbf{Simplest Tagger}: Tag word based on its \textit{individual} most common POS

  \subsection*{\blue{Hidden Markov Model}}

  \begin{align*}
    \argmax_{\vec{y}} \Pr\sbrack{\vec{y} \vert \vec{x}} &= \argmax_{\vec{y}} \frac{\Pr\sbrack{\vec{x} \vert \vec{y}}\Pr[\vec{y}]}{\Pr[\vec{x}]} \\
                                                        &= \argmax_{\vec{y}} \Pr\sbrack{\vec{x} \vert \vec{y}} \Pr[\vec{y}]
  \end{align*}

  \greenbf{First-Order Markov Assumption}:
  \begin{equation*}
    \Pr\sbrack{y_{1},\ldots,y_{n}} = \prod_{i=1}^{n} \Pr[y_{i} \vert y_{i-1}]
  \end{equation*}

  \greenbf{Indepen.\ Assum} $\Pr\sbrack{\vec{x} \vert \vec{y}} = \prod_{i=1}^{n} \Pr[x_{i} \vert y_{i}]$

  \greenbf{Transition Prob}: $\Pr\sbrack{y_{t} \vert y_{t-1}}$

  \greenbf{Emission Prob}: $\Pr\sbrack{x_{t} \vert y_{t}}$

  \subsubsection*{\red{Decoding}}

  \greenbf{Greedy Decoding}: \\${y_t = \argmax_{y'} \Pr[x_t \vert y'] \Pr[y' \vert y_{t-1}]}$

  \greenbf{\hl{Viterbi}}
  \begin{itemize}
    \item \textit{Complexity}: $O(\abs{S}^{2} n)$
    \item \textbf{Rows}: States, \textbf{Columns}: ${0,\ldots,n+1}$
    \item Can solve HMM \& CRF
  \end{itemize}

  \begin{equation*}
    v_{t}(s) = \max_{s' \in Y}\set{v_{t-1}(s') \Pr[s \vert s'] \Pr[x_t \vert s]}
  \end{equation*}

  \subsection*{\blue{Max.\ Entropy Markov Model}}
  \begin{align*}
    \Pr[y\vert x] &= \prod_{i=1}^{n} \Pr(y_t \vert y_{t-1},x) \\
    \Pr[y_{t} \vert y_{t-i}, x] &= \frac{\exp\left(\sum_{i=1}^{K}w_{i}^{y_t}f_{i}(y_{t-1},x) \right)}{Z(y_{t-1},x)}
  \end{align*}
  \begin{equation*}
    L(\theta) = -\sum_{x,y\in D} \log \Pr(y \vert x)
  \end{equation*}

  Both MEMM \& CRF model $\Pr(y\vert x)$
  \greenbf{Local Norm}: Normalize at each state (MEMM). \redbf{Label bias} \greenbf{Global Normalization}: Normalizes over entire input. Leads to CRF\@.

  \subsection*{\blue{Conditional Random Field}}
  \textbf{Complexity}: $O(n\abs{Y}^{2})$ where $Y$ set label vectors
  \begin{equation*}
    \Pr[y \vert x; \theta] = \frac{\exp(\Phi(\vec{x},\vec{y})\transpose\theta))}{\sum_{\vec{y}' \in Y} \exp(\Phi(\vec{x}, \vec{y'})\transpose\theta)}
  \end{equation*}

  where:
  \begin{align*}
      \Phi(\vec{x},\vec{y}) &= \sbrack{\Phi_{1}(\vec{x},\vec{y}),\ldots,\Phi_{K}(\vec{x},\vec{y})} \\
      \Phi_{k}(\vec{x},\vec{y}) &= \sum_{i=1}^{n} \phi_{k}(y_{i-1},y_{i},\vec{x},i)
  \end{align*}
  $\Phi_{k}(\vec{x},\vec{y})$: Captures features of transition from $y_{i-1}$ to $y_i$

  \subsection*{\blue{Bidirectional RNN}}

  \begin{itemize}
    \item \textbf{Input}: Data vec \& previous hidden vec
    \item Can incorporate CRF as final layer in sequence labeling
  \end{itemize}

  \section*{Lecture~\#6: \hl{Constituent Parsing}}

  \greenbf{Constituency}: Group words behave single unit

  \greenbf{Substitution Test}: If a constituent is replaced by another constituent of same type, does sentence remain \textit{grammatical}

  \greenbf{Context-Free Grammar} Formal def.\ meaningful constituents \& how constituent is formed from other constituents.  \textit{Every internal tree node is a phrase that can be replaced by another of same type of constituent}
  \begin{itemize}
    \item $N$: Set of non-terminal symbols
    \item $\Sigma$: Alphabet terminal symbols
    \item $R$: Set of production rules $A \rightarrow \beta$
    \item $S$: Start symbol
  \end{itemize}

  \textbf{Language}: All strings derivable from $S$

  \textbf{Preterminals}: Rewrite as terminals if possible

  \textbf{Syntax}: Move from labeling discrete items to structure between items

  \greenbf{Attachment Ambiguity}: Constituent can attach to parse tree at more than one place
  \begin{itemize}
    \item ``I saw the man with a telescope''
  \end{itemize}

  \greenbf{Coordination Ambiguity} Diff sets phrases can be conjoined by conjunction like ``and''
  \begin{itemize}
    \item ``Old man and woman''
  \end{itemize}

  \greenbf{Treebank}: Collection of sentences annotated with syntactic structure

  \greenbf{Chomsky Normal Form} (CNF): All rules have form ${A \rightarrow \beta}$ where $\beta$~either single terminal in~$\Sigma$ or two non-terminals in~$N$
  \begin{itemize}
    \item Fix mixed terminals/non-terminals
    \item Fix more than 2 non-terminals
    \item Fix single non-terminal
  \end{itemize}

  \subsection*{\blue{\hl{CYK Parsing}}}

  \begin{itemize}
    \item \red{Left-to-right, bottom-up}
    \item \red{Every time has below it a range ${[start,end)}$}
    \item Checks whether sentence grammatical in CFG's language
    \item Enumerate all poss.\ parses for sentence
  \end{itemize}

  \greenbf{Probabilistic Context-Free Grammar}: Tells which parse of sentence more likely
  \begin{itemize}
    \item Each production associated w.\ prob.
  \end{itemize}

  \redbf{Strong Independence Assumption} where ${\Pr(T) = \prod_{i=1}^{n} \Pr(\beta \vert A)}$. In practice, productions strongly dependent on place in tree.

  \textbf{Improving PCFG}
  \begin{itemize}
    \item Different probabilities for based on whether subject/object
    \item \greenbf{Lexicalized PCFG} include POS \& word in calc transition probabilities
  \end{itemize}
  \begin{equation*}
    \Pr(\beta \vert A) = \frac{\text{Count}(A \rightarrow \beta)}{\text{Count}(A)}
  \end{equation*}

  \subsection*{\blue{Evaluation}}

  Tuple ${\langle l_k, i_k, j_k  \rangle}$ where $l_k$ label for $k$\-/th phrase, $i_k$ start index $k$\-/th phrase, $j_k$ end index $k$\-/th phrase
  \begin{itemize}
    \item Tuple correct if all elements in tuple match correct tree
  \end{itemize}

  \section*{Lecture~\#7: \hl{Dependency Parsing}}

  \greenbf{Dependency Structure}: Directed graph consisting of set of vertices $V$ and arcs~$A$

  \textit{Approaches}: DP, graph alg, CSP, transition-based parsing

  Most depend parsing algs only handle projective tree

  Closer to semantic relations than constituents

  Except root, exactly one incoming edge. Tail is dependent on head word. Arcs have a type

  Not tied linear word order so more language syntax independent. \textit{Free word order}

  \textbf{DP Complexity}: $O(n^3)$ (Eisner's)

  \subsection*{Greedy Dependency Parsing}

  \greenbf{Guide}: Predict transition given current config

  \greenbf{\hl{Arc-Standard Algorithm}}

  \textbf{Transition-based parsing}

  No guarantee finds the best tree.

  \textbf{Complexity}: $O(n)$ Projective, $O(n^2)$

  \textbf{Configuration}: Buffer, stack \& dependency tree

  \textbf{Init Config}: Stack \& tree empty Words in buf

  \textbf{Term Config}: Buffer empty. Stack is single word

  \textbf{\#Classes}: 1 + 2 * \# arc type

  \redbf{Operations}
  \begin{itemize}
    \item \texttt{shift}: Move from buffer to top of stack
    \item \texttt{left-arc}: Add arc from top of stack to penultimate on stack. Pop penultimate.
    \item \texttt{right-arc}: Add arc from penultimate of stack to top. Pop top.
    \item \texttt{swap}: Only if non-projective
  \end{itemize}

  \redbf{Problem}: Feature calculation slow so slow use DNN.

  \greenbf{Projective}: Has directed path between head of arc and all words between head and tail. \textit{No crossing dependency arcs}

  \subsection*{\blue{Dependency Evaluation}}

  \greenbf{Labeled Attachment Score} (LAS): Percentage of correct arcs relative to gold

  \greenbf{Labeled Exact Match} (LEM): Percentage of correct dependency trees

  \greenbf{Unlabeled Attachment Score/Exact Match} (UAS/UEM): Same as above but ignore arc labels

  \textbf{Word-Level}: Microaverage (More common) \textbf{Sentence-Level}: Macroaverage

  \section*{Lecture~\#8: \hl{IE \& RE}}

  \greenbf{Information Extraction}: Automatic extraction of structured information from unstructured or semi-structured docs
  \begin{itemize}
    \item Returns \textit{facts} from documents
  \end{itemize}

  \greenbf{Information Retrieval}: Returns a set of documents given a query

  \greenbf{Relation Extraction}: Task extract semantic relationships from text
  \begin{itemize}
    \item \redbf{Relation}: Predication about pair
    \item \redbf{Position Embedding}: Distance between pair of entities is its own embedding
    \item \redbf{DNN Input}: Shortest dependency path between two entities
    \item \redbf{GCN}: Run over dependency tree.
    \item \textbf{Features}: Head word of entity1, head word entity2, word before entity1, word after entity1, type entity1
  \end{itemize}

  \greenbf{Co-Reference Resolution}: Find all expressions that refer to same entity in text

  \greenbf{Entity Linking}: Task recognize \& disambiguate named entities to a knowledge base

  \redbf{Trigger Prediction}: Indicate event start

  \redbf{Argument Prediction}: After trigger found, identify objects associated with trigger

  \greenbf{Gazetteer}: List common names for diff type

  \textbf{Transformations}
  \begin{itemize}
    \item Remove passive voice
    \item Remove relative (Harry\st{, who})
    \item Subject control to duplicate subject for dependent phrase
  \end{itemize}

  \greenbf{String Kernel}: Two strings more similar if share more substrings.

  \greenbf{Tree Kernel}: Meas similarity btwn two tree. Based on number common subtrees.

  \greenbf{Confidence}
  \begin{itemize}
    \item \textit{Binary Classification}: $2(P-0.5)$
    \item \textit{Multi-class}: $P_1 - P_2$
    \item \textit{SVM}: Distance from hyperplane
  \end{itemize}

  \subsection*{\blue{Evaluation}}

  \greenbf{NER}: Tuple ${\langle i_k, j_k, l_k \rangle}$ for start, end, \& type

  \subsection*{\blue{Semi-Supervised Learning}}

  \greenbf{Semantic Drift}: In iterative learning, target may shift from target class to other class

  \textbf{Termination Conditions}: 1) $U$ exhausted 2) Performance goal hit 3) Fixed \#iteration

  \redbf{Problem}: Errors in early greedy confidence choices rapidly magnified

  \subsection*{\blue{Co-Training}}
  Two \textit{views} of data (subset features). Alternate training separate classifiers on feature subsets

  \subsection*{\blue{Active Learning}}
  Select `informative' examples for user label. \textit{Simulated active learning}. Slow since need to retrain after each point. May not generalized to non-well match datasets. \textit{Representativeness}

\end{multicols}


\begin{figure}[t]
  \centering
  \begin{minipage}{.5\textwidth}
    \begin{algorithm}[H]
      \small
      \caption{CYK algorithm}
      \begin{algorithmic}[1]
        \For{$j \gets 1 \text{ \textbf{to} } \textproc{Length}(words)$}
          \ForEach{$\set{A \vert A \rightarrow words[j] \in gram}$}
            \State $tab[j-1,j] \gets tab[j-1,j] \cup A$
          \EndFor
          \For{$i \gets j-2 \text{ down to } 0$}
            \For{$k \gets i+1 \text{ to } j - 1$}
              \ForEach{$\set{A \vert A \rightarrow BC \in gram \wedge B \in tab[i,k] \wedge C \in tab[k,j]}$}
                \State $tab[i,j] \gets tab[i,j] \cup A$
              \EndFor
            \EndFor
          \EndFor
        \EndFor
      \end{algorithmic}
    \end{algorithm}
  \end{minipage}%
  \begin{minipage}{.5\textwidth}
    \begin{algorithm}[H]
      \small
      \caption{Probabilistic CYK algorithm}
      \begin{algorithmic}[1]
        \For{$j \gets 1 \text{ \textbf{to} } \textproc{Length}(words)$}
          \ForEach{$\set{A \vert A \rightarrow words[j] \in gram}$}
          \State $tab[j-1,j, A] \gets \Pr(A \rightarrow words[j])$
          \EndFor
          \For{$i \gets j-2 \text{ \textbf{down to} } 0$}
          \For{$k \gets i+1 \text{ \textbf{to} } j - 1$}
              \ForEach{$\set{A \vert A \rightarrow BC \in gram \wedge tab[i,k, B] > 0 \wedge tab[k,j,C] > 0}$}
                \If{$tab[i,j,A] < \Pr(A \rightarrow BC) \times tab[i,k,B] \times tab[k,j,C]$}
                  \State $tab[i,j,A] \gets \Pr(A \rightarrow BC) \times tab[i,k,B] \times tab[k,j,C]$
                \EndIf
              \EndFor
            \EndFor
          \EndFor
        \EndFor
      \end{algorithmic}
    \end{algorithm}
  \end{minipage}
\end{figure}
\begin{table}[t]
  \centering
  \begin{tabular}{|c|l|c|l|c|l|}
    \hline
    \bluebf{nsubj} & Subject of verb & \bluebf{obj} & Direct object of verb & \bluebf{iobj} & Indirect object of verb \\\hline
    \bluebf{obl} & Oblique modifier of verb & \bluebf{ccomp} & Clausal complement & \bluebf{advcl} & Adverbial clause modifier \\\hline
    \bluebf{acl} & Clausal modifier of noun & \bluebf{conj} & Conjunct in coordination & & \red{Verb usually head of arc} \\\hline
  \end{tabular}
\end{table}
\begin{table}[b]
  \centering
  \begin{tabular}{|c|l|c|l|}
    \hline
    \textbf{JJ} vs.\ \textbf{VBN} & \multicolumn{3}{l|}{If gradable (can add ``very''), then \texttt{JJ}. If can be followed by a ``by'' phrase, \texttt{VBN}} \\\hline
    \textbf{JJ} vs.\ \textbf{NP}/\textbf{NPS} & \multicolumn{3}{l|}{Proper names can be adjectives or nouns (\textit{French})} \\\hline
    \textbf{NN} vs.\ \textbf{VBG} & \multicolumn{3}{l|}{Only adj.\ can modify \texttt{NN} (Good \textit{cooking}). Only gerunds can be modified by adverbs (\textit{Cooking} well).} \\\hline
    \textbf{IN} vs.\ \textbf{RP} & \multicolumn{3}{l|}{If \green{can} precede or follow noun phrase, \texttt{RP}. If it \green{must} precede noun phrase, \texttt{IN} (move word end of phrase \& is syntactical)} \\\hline
    \bluebf{PRP} Personal Pronoun & \multicolumn{3}{l|}{\textit{I}, \textit{me}, \textit{you}, \textit{he}, \textit{him}, \textit{it}, Reflective pronoun end in \texttt{\red{-self}} (\textit{himself}) nominal possessive pronoun (\textit{mine}, \textit{yours}, \textit{hers})} \\\hline
    \bluebf{DT} Determiner & \multicolumn{3}{l|}{Articles (\textit{a}, \textit{the}, \textit{every}, \textit{no}), \textit{that}, \textit{these}, \textit{this}, \textit{those} precede noun. \textit{All}, \textit{both} not precede \texttt{DT} or \texttt{PRP\$}}  \\\hline
    \bluebf{PDT} Predeterminer & Precede article or possessive pronoun & \bluebf{VB} Base Form & In imperative, infinites, subjectives \textit{do}\\\hline
    \bluebf{CC} Coord.\ Conjunction & \multicolumn{3}{l|}{\textit{And}, \textit{but}, \textit{not}, \textit{or}, math operators (\textit{plus}, \textit{minus}, \textit{less}, \textit{times}), \textit{For} meaning because} \\\hline
    \bluebf{RB} Adverb & \multicolumn{3}{l|}{Most words end in \texttt{\red{-ly}}, degree words (\textit{quite}, \textit{too}, \textit{very}), negative markers (\textit{not}, \textit{n't}, \textit{never})} \\\hline
    \bluebf{VBP} Present & Non-3rd.\ I \textit{am}, You \textit{are} & \bluebf{VBZ} Third Person & She \textit{is}. He \textit{likes} \\\hline
    \bluebf{VBG} \green{Present} Participle & Verb functions as noun ending \texttt{\red{-ing}} & \bluebf{VBN} \red{Past} Participle & ends in \texttt{\red{-en}} or \texttt{\red{-ed}} \\\hline
    \bluebf{RBR} Comparative Adv. & Adverb end in \texttt{\red{-er}}. \textit{More}, \textit{Less} & \bluebf{RBS} Superlative Adv. & Adverb end in \texttt{\red{-est}}. \textit{Most}, \textit{Least} \\\hline
    \bluebf{RP} Particle & \textit{over}, \textit{down}, \textit{out}, \textit{on}, \textit{up}, \textit{off} & \bluebf{JJ} Adjective & General adj., ordinal number (\textit{fourth}) \\\hline
    \bluebf{JJR} Comparative Adj. & Adjective end in \texttt{\red{-er}}. \textit{More}, \textit{less} & \bluebf{JJS} Superlative Adj. & Adjective end in \texttt{\red{-est}}. \textit{Most}, \textit{least} \\\hline
    \bluebf{PRP\$} Possess.\ Pronoun & \textit{my}, \textit{their}, \textit{its}, \textit{his}, \textit{her} & \bluebf{IN} Preposition & All preposition except ``to''. \textit{On}, \textit{because}, \textit{from} \\\hline
  \end{tabular}
\end{table}
\end{document}
