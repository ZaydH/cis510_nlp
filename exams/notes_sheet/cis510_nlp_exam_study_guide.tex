\documentclass[9pt]{extarticle}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi)
% \usepackage{fullpage} % changes the margin
\usepackage[margin=0.3in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{multicol}

\usepackage{subcaption}

\usepackage{titlesec}  % Used to adjust title heading
% Format:  \titlespacing{command}{left spacing}{before spacing}{after spacing}[right]
\titlespacing\section{0pt}{6pt plus 4pt minus 0pt}{4pt plus 2pt minus 0pt}

\setlength\parindent{0pt}
\setlength\itemsep{0pt}

\newcommand{\horizontalbreak}{
  {
    \begin{center}
      \noindent\rule{7.5in}{0.4pt}
    \end{center}
  }
}

\input{global_macros}
\usepackage[dvipsnames]{xcolor}
\renewcommand{\green}[1]{{\color{ForestGreen} #1}}

\begin{document}
\begin{center}
CIS510 NLP Exam Notes Sheet -- Zayd Hammoudeh
\end{center}
\begin{multicols}{3}
  \section*{Lecture~\#1: Introduction}

  \textbf{Challenges of NLP}:
  \begin{itemize}
    \item Language is \textit{discrete}. Many CV techniques do not work
    \item Language is \textit{compositonal}. Meaning comes form understanding individual words \& how they combine
    \item \textit{Ambiguity}:
  \end{itemize}

  \section*{Lecture~\#2: Text Classification}
  \textit{Examples}: Topic, sentiment, language, authorship

  \begin{equation*}\label{eq:L02:NaiveBayes}
    \argmax_c \Pr\sbrack{c \vert X} = \argmax_{c} \frac{\Pr[X \vert c] \Pr[c]}{\Pr[X]}
  \end{equation*}
  \begin{equation*}\label{eq:L02:Bernoulli}
    \Pr[c] = \frac{\text{\#docs label } c \text{ in } D}{\text{\#docs in } D}
  \end{equation*}

  \section*{Lecture~\#3: Word Embeddings}
  \textit{Distributional Semantics}: A word's meaning is given by words frequently appear close-by.

  \textit{Context}: Set of words that appear nearby in fixed-size window

  \textbf{\blue{Continuous Bag of Words}} (CBOW) ${\Pr\sbrack{w_t \vert w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}}}$

  \textbf{\blue{Skip-Grams}} (SG): \\ ${\Pr\sbrack{w_{t+i} \vert w_{t}}, i \in \set{-2,-1,1,2}}$

  \section*{Lecture~\#4: Deep Learning}
  \textbf{Logistic Regression}:
  \begin{equation*}\label{eq:L04:LogReg}
    \mathcal{L}(\mathbf{w},b) = -\sum_{\mathbf{x}_i,y_i \in \mathbf{d}} \log \sigma\big( y_i(\mathbf{w}\cdot \mathbf{x}_i + b) \big)
  \end{equation*}
  \section*{Lecture~\#5: Sequential Labeling}
  \subsection*{\blue{Hidden Markov Model}}

  \begin{align*}
    \argmax_{\vec{y}} \Pr\sbrack{\vec{y} \vert \vec{x}} &= \argmax_{\vec{y}} \frac{\Pr\sbrack{\vec{x} \vert \vec{y}}\Pr[\vec{y}]}{\Pr[\vec{x}]} \\
                                                        &= \argmax_{\vec{y}} \Pr\sbrack{\vec{x} \vert \vec{y}} \Pr[\vec{y}]
  \end{align*}

  \textbf{\green{First-Order Markov Assumption}}:

  \begin{equation*}
    \Pr\sbrack{y_{1},\ldots,y_{n}} = \prod_{i=1}^{n} \Pr[y_{i} \vert y_{i-1}]
  \end{equation*}

  \textbf{\green{Indepen.\ Assum}} $\Pr\sbrack{\vec{x} \vert \vec{y}} = \prod_{i=1}^{n} \Pr[x_{i} \vert y_{i}]$

  \textbf{\green{Transition Prob}}: $\Pr\sbrack{y_{t} \vert y_{t-1}}$

  \textbf{\green{Emission Prob}}: $\Pr\sbrack{x_{t} \vert y_{t}}$

  \subsection*{Max.\ Entropy Markov Model}
  \subsection*{CRF}
  \section*{Lecture~\#6: Constituent Parsing}
  \section*{Lecture~\#7: Dependency Parsing}

\end{multicols}


\end{document}
