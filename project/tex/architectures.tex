\section{Double decoder PU~learning}\label{sec:ddPU}



\subsection{Autoencoder pretraining}



\input{alg/pretrain.tex}

\subsection{\Repel\ training}

\input{alg/pu_training.tex}

\subsection{Inference}

% For unlabeled example~${\X \in \unlabel}$, if $\fPUp$ yields a superior reconstruction than $g_{n}$, it can be reasonably concluded that $\X$~is positive labeled; otherwise, $\X$ is more likely negative labeled.  This intuition is the basis for \toolname's inference function shown in Eq.~\eqref{eq:PU:ClassificationFunc}.

%   % \begin{equation}\label{eq:PU:ClassificationFunc}
%   %   \hat{y} = -\sign{\norm}
%   % \end{equation}

% \noindent
% In rare cases where ${\norm{} = \norm{}}$, $\X$ is equally likely to be either negative or positive labeled so it can be assigned either label.

\subsection{A probabilistic perspective}

Section~\ref{sec:Siamese} discussed that the similarities
