\section{Classifier neural architectures}\label{sec:Architectures}

Risk estimators are generally classifier agnostic.  They can be coupled with whatever learning paradigm imparts the most advantageous inductive bias. When labeled data is limited (including being non\-/existent for some classes), transfer learning often improves a classifier's performance by leveraging portable feature information learned from other (labeled) datasets.

This section proposes two 20~newsgroups neural classifier architectures.  Section~\ref{sec:Architectures:LSTM}'s LSTM model only makes limited use of transfer learning but is more flexible overall.  Section~\ref{sec:Architectures:ELMo}'s ELMo~model depends almost entirely on transfer learning and is more rigid. A comparison of each architecture's empirical results is provided in Section~\ref{sec:ExperimentalResults}.

\subsection{End-to-end LSTM network}\label{sec:Architectures:LSTM}

This architecture is the ``classic'' deep recurrent NLP classifier. Document tokens are encoded using an embedding (e.g,~GloVe), and these dense representations are input to a recurrent block composed of one or more bidirectional LSTM layers.  The final recurrent block output is passed to a feedforward network with one or more hidden layers and ReLU activation.

This model's only transferred knowledge is the (GloVe)~embedding matrix. All LSTM and feedforward parameters must be learned from scratch.  Given the limited labeled data, this may be onercous but as a positive it allows the learner significant flexibility.  We experimented with freezing and unfreezing the embedding weights, but saw no major performance difference with either approach.

\subsection{ELMo preprocessed vectors}\label{sec:Architectures:ELMo}

The traditional word embedding model treats each token (word) as context independent and yield token-specific representations. Proposed in~2018 by Peters\etal~\cite{Peters:2018}, ELMo (\underline{e}mbeddings for \underline{l}anguage \underline{mo}dels) extends the word embedding model such that each token representation is \textit{context dependent} --- a function of the current token as well as the entire preceding input.  ELMo uses unsupervised learning allowing it to be pretrained on a large scale.  ELMo representations easily integrate into many existing NLP~architectures, and often can be swapped for GloVe or other word embeddings.

The Allen Institute for AI provides multiple pretrained ELMo models.  We selected the best performing ELMo network --- the original architecture trained on the extended 5.5~billion token corpus, specifically 1.9~billion tokens from Wikipedia and 3.6~billion from the WMT monolingual news crawl. At over 93~million parameters, combining this ELMo model with long 20~newsgroups documents causes GPU memory overflow errors on the University of Oregon's K80~GPUs.  This limited our of use of ELMO to just creating a single static vector for each document --- in essence a ``document embedding.''

The original ELMo embedder model has three layers --- a character CNN (for improved unknown word robustness) followed by two bidirectional LSTMs.  The layer's output is a 1,024\-/dimension vector.  Given an input stream of $m$~tokens, the embedder's output is a tensor of size~${\langle \textnormal{\#Layers} \times d_{\textnormal{layer}} \times \textnormal{\#Tokens} \rangle}$ --- in this case~${\langle 3 \times 1024 \times m \rangle}$.

We used the R\"{u}ckl\'{e}\etal's~\cite{Ruckle:2018} paradigm where a sentence representation is formed by taking the minimum, maximum, and average value along each layer output dimension in a tensor like above. Therefore, our model turns each document into a vector of dimension ${\abs{\set{\max,\min,\textnormal{avg}}} \cdot \textnormal{\#Layers} \cdot d_{\textnormal{layer}} = 3 \cdot 3 \cdot 1024 = 9,216}$.

Each aforementioned static document representation is input into a feedforward network with two hidden layers of dimension~300 and ReLU~activations. This feedforward block contains the only volatile parameters making each epoch very fast~(<500ms in our experiments).\footnote{This ELMo\-/based architecture was first proposed by Hsieh\etal~\cite{Hsieh:2018}.}

\subsection{Implementation}

Irrespective of the classifier architecture, each newsgroup document was tokenized using~\texttt{nltk}. Both architectures are implemented in PyTorch with the source code available on~\href{\sourceCode}{Github}.  In our implementation, the LSTM~architecture is the default. To instead use preprocessed ELMo~vectors, set command line flag~\texttt{\mbox{\-/\-/preprocessed}}.
