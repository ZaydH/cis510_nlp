\section{Experimental results}\label{sec:ExperimentalResults}

We compare the performance of the PN, nnPU, and~PUbN risk estimators on both proposed neural architectures.  The experiments are modeled after the setup in~\cite{Hsieh:2018}.

For each labeled set (e.g.,~$\ptrain$, $\bntrain$,~$\ntrain$), 500~training examples were sampled from the corresponding conditional distribution.  The unlabeled training set consisted of 6,000~elements.

All learners were trained for 50 epochs. A disjoint validation set -- one-fifth the training set's size -- was used to select the best epoch. The training set's risk used the logistic loss.  The validation's set risk used the sigmoid loss to ensure no single example has an outsized impact on model selection.

The hyperparameters we tuned were learning rate ${\alpha \in \set{5 \cdot 10^{-3}, 10^{-3}, 5 \cdot 10^{-4}}}$, ${\tau \in \set{0.5, 0.7, 0.9}}$, and ${\gamma \in \set{0.1, 0.3, 0.5, 0.7, 0.9, 1}}$.  Each architecture and risk estimator pairing's optimal hyperparameter setting was selected using a grid search to find the minimum overall validation loss.

\subsection{Baselines}

The PN~learners serve as the performance baselines.  Unbiased~PN -- where ${\ptrain \sim \pcond}$ and ${\ntrain \sim \ncond}$ -- represents each architecture's performance ceiling given sizes~$\abs{\ptrain}$ and~$\abs{\ntrain}$.

Biased~PN follows an identical training procedure to its unbiased counterpart with the exception that ${\bntrain \sim \bncond}$~is used instead of $\ntrain$.  Since biased~PN is the simplest approach -- both theoretically and in implementation -- one would expect it would be handily outperformed by slower, more advanced risk estimators like~nnPU and~PUbN.

\subsection{Selection bias profiles}

Recall that under covariate shift, only the marginal distributions change.  As shown in Table~\eqref{tab:ExperimentalResults}, we achieve this by modifying the prior probabilities of each of the three negative categories.  The extent to which each category is biased can be determined by comparing Table~\ref{tab:ExperimentalResults}'s biases to Table~\ref{tab:20newsgroups}.

The selection biases we tested were of two variants -- each stressing different estimator limitations.  In the first two experiments, the biased negative set is drawn from only one negative category, i.e.,~sci.\ and~talk.\ respectively.  The second bias variant draws training examples from all negative categories but shifts the prior probabilities.

nnPU is unaffected by the biased negative set since it considers only~$\ptrain$ and~$\utrain$.  Unbiased~PN (by definition) does not consider~$\bntrain$ so it too is unaffected.  Therefore, only a single result per architecture is reported for those two risk estimators.

\subsection{Architecture comparison}

Tables~\ref{tab:ExperimentalResults:LSTM} and~\ref{tab:ExperimentalResults:ELMo} enumerate the test set (inductive) accuracy results for the~LSTM and preprocessed\-/ELMo architectures respectively.  Across all risk estimators and selection biases, the ELMo~preprocessed architecture performed better -- often substantially -- than the LSTM model as shown in Table~\ref{tab:ExperimentalResults:Comparison}.

ELMo's performance advantage most likely derives from its more extensive leveraging of transfer learning.  This indicates that when dealing with limited or biased labeled data, transfer learning's importance may compound.

We exclusively discuss the ELMo architecture going forward due to its vastly superior performance.

\subsection{Risk estimator comparison}

Table~\ref{tab:ExperimentalResults:ELMo}'s first two rows show that when the biased negative set is drawn from only a single negative category, PUbN had the best performance.  When the biased negative set was drawn from all negative categories, biased~PN had the best results.

To summarize formally, the most important factor in determining the best risk estimator is the extent to which the negative class-conditional distribution's support is covered.  Selecting all labeled examples from a single categories entails that a PN~learner has no guidance on how to classify examples from the remaining two negative categories.  That is why biased~PN performed so poorly in these tests.

In contrast when $\bntrain$~has labeled examples from all categories, biased~PN is still able to learn how to predict all categories and can outperform even classifiers that are provided information about the selection bias like~PUbN.

\begin{table}[t]
  \caption{20~newsgroups negative covariate shift test set accuracy results for the two classifier architectures and three bias configurations. Listed below each category name is its biased prior probability in that experiment. The corresponding labeling probability~$\plabel$ is also provided. The best performing learners are bolded.}\label{tab:ExperimentalResults}
  \begin{subtable}[t]{\textwidth}
    \centering
    \caption{End-to-end LSTM architecture results. Due to the longer LSTM training time and limited computational resoruces, the grid search used only a single trial per hyperparameter setting.}\label{tab:ExperimentalResults:LSTM}
    \input{tables/lstm_results.tex}
  \end{subtable}

  \begin{subtable}[t]{\textwidth}
    \centering
    \caption{Preprocessed\-/ELMo architecture results averaged across 10~independent trials}\label{tab:ExperimentalResults:ELMo}
    \input{tables/elmo_results.tex}
  \end{subtable}

  \begin{subtable}[t]{\textwidth}
    \centering
    \caption{Performance difference between the preprocessed-ELMo \& LSTM results}\label{tab:ExperimentalResults:Comparison}
    \input{tables/results_compare.tex}
  \end{subtable}
\end{table}
