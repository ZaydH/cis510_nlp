\documentclass[]{subfiles}

\begin{document}
\section{Introduction}\label{sec:Introduction}

Consider binary classification of text documents.  Each document is represented by two random variables: independent feature vector~${\X \in \domainX}$ and dependent label~${\y \in \domainY}$. The document population is generated from an unknown, joint probability distribution~${\joint(\X,\y)}$.  By Bayes' rule, the joint distribution decomposes as

\begin{align}
    \joint(\X,\y) &= \marginal(\X) \posterior \label{eq:TrainDist} \\
                  &= \pDist(\y = \pcls) \pDist(\X \vert \y = \pcls) + \pDist(\y = \ncls) \pDist(\X \vert \y = \ncls) \nonumber\\
                  &\equiv \prior \pcond(\X) + (1 - \prior) \ncond(\X) \label{eq:Joint:Bayes}
\end{align}

\noindent
where $\prior$~is the positive class prior probability while $\pcond$~and $\ncond$~are the positive and negative class-conditional distributions respectively.

Supervised binary classification's training set,~$\train$, traditionally consists of $\size$~independent samples from Eq.~\eqref{eq:Joint:Bayes}.  Therefore, the training set partitions as ${\train = \ptrain \sqcup \ntrain}$ where ${\ptrain \sim \pcond}$ are the positive-valued examples and ${\ntrain \sim \ncond}$ are negative-valued.  For that reason, this training paradigm is often referred to as \textit{positive-negative}~(PN) learning.

This idealized supervised training model often does not apply in practice.  Training set labeling can be expensive or difficult meaning there are few labeled examples but numerous unlabeled ones.  Additionally, the labeled set may not be representative of~$\joint$.  For example, the training examples only characterizes a small subset of $\joint$'s support, including where there is no labeled data at all for one class.

The \textit{empirical risk minimization} training framework assumes that a low training set expected risk correlates to low inference error. Training set bias, such as those aforementioned, undermine that assumption, and can lead to large, unpredictable test set error rates.

One of the most common types of training set bias is \textit{covariate shift}. Define $\joint$~\eqref{eq:TrainDist} and ${\joint'}$~\eqref{eq:TestDist} as the training and set joint distributions respectively.  Posterior distribution,~${\posterior}$ is identical in both cases; rather, they only differ in their marginal distributions, i.e.,~$\marginal$ and~${\marginal'}$.~\cite{Huang:2006}  This paper assumes covariate shift for only one class, i.e.,~the negative one.

\begin{equation}\label{eq:TestDist}
    \joint'(\X,\y) = \marginal'(\X) \posterior
\end{equation}

Previous work has taken different routes to overcome negative covariate shift's effects on text classification.  Li\etal~\cite{Li:2010} ignored the biased negative training data entirely and attempted to learn a classifier using only the positive and unlabeled sets -- an approach known as \textit{positive\-/unlabeled}~(PU) learning.  Fei \&~Liu~\cite{Fei:2015} took the alternate approach of ignoring any unlabeled data and learned a traditional supervised classifier using just the positive and biased-negative training examples.  Hsieh\etal~\cite{Hsieh:2018} jointly used the \underline{p}ositive, \underline{u}nlabeled, and \underline{b}iased \underline{n}egative~(PUbN) in a risk estimator that works with standard empirical risk minimization.

The primary contribution of this work is an empirical comparison of PN, PU, and~PUbN classifiers on the 20~newsgroups dataset~\cite{20newsgroups} under negative covariate shift.  The remainder of this document is structured as follows.  Section~\ref{sec:RiskEstimators} describes the PN, PU, and~PUBN risk estimation functions. Section~\ref{sec:Architectures} describes the two neural network architectures used. Section~\ref{sec:20newsgroups} provides a brief overview of the 20~newsgroups dataset including how it is used in this paper. Section~\ref{sec:ExperimentalResults} describes our experimental setup, baselines, and results.  We provide brief concluding comments in Section~\ref{sec:Conclusions}.
\end{document}
