\documentclass[]{subfiles}

\begin{document}
\section{Introduction}\label{sec:Introduction}

Consider binary classification of text documents.  Each document is represented by two random variables: independent feature vector~${\X \in \domainX}$ and dependent label~${\y \in \domainY}$. The document population is generated from an unknown, joint probability distribution~${\joint \equiv \pDist(\X,\y)}$, which by Bayes' rule can be reformulated as

\begin{align}
  \underbrace{\pDist(\X,\y)}_{\joint} &= \pDist(\X) \posterior \label{eq:TrainDist} \\
           &= \underbrace{\pDist(\y = \pcls)}_{\prior} \underbrace{\pDist(\X \vert \y = \pcls)}_{\pcond} + \underbrace{\pDist(\y = \ncls)}_{1-\prior} \underbrace{\pDist(\X \vert \y = \ncls)}_{\ncond} \label{eq:Joint:Bayes}
\end{align}

\noindent
where $\prior$~is the positive\-/class prior while $\pcond$~and $\ncond$~are the positive and negative class-conditional distributions respectively.

Supervised binary classification's training set,~$\train$, consists of samples generated from~$\joint$. By Eq.~\eqref{eq:Joint:Bayes}, $\train$~partitions as ${\ptrain \sqcup \ntrain}$ where ${\ptrain \sim \pcond}$ are the positive-valued examples and ${\ntrain \sim \ncond}$ are negative-valued.  That is why supervised learning is alternatively referred to as \textit{positive\-/negative}~(PN) learning.

Ideal supervised learning often does not apply in practice.  Labeling may be expensive or difficult resulting in few labeled examples but numerous unlabeled ones.  Additionally, the labeled set may not be representative of~$\joint$.  For example, labeled examples may only characterize a small subset of $\joint$'s support, including there being no labeled data at all for one class.

One of the most common types of training set bias is \textit{covariate shift}. Define $\joint$~\eqref{eq:TrainDist} and ${\joint'}$~\eqref{eq:TestDist} as the training and test joint distributions respectively.  In covariate shit, posterior distribution,~${\posterior}$ is identical in both joint distributions. Rather, the distributions only differ in their respective marginals, i.e.,~${\marginal \equiv \pDist(\X)}$ and~$\marginal'$.~\cite{Huang:2006}  This paper assumes covariate shift for only one class --~the negative one.

\begin{equation}\label{eq:TestDist}
  \joint'(\X,\y) = \underbrace{\pDist'(\X)}_{\marginal'} \posterior
\end{equation}

The \textit{empirical risk minimization}~(ERM) training framework assumes that a low training-set expected risk correlates to low inference error. Training set bias, such as those aforementioned, undermine that assumption and can lead to large, unpredictable test set error rates.

Previous work has taken different approaches to overcome negative covariate shift's effects on text classification.  Li\etal~\cite{Li:2010} ignored the biased-negative training data entirely and learned a classifier using only the positive and unlabeled sets -- a paradigm known as \textit{\underline{p}ositive\-/\underline{u}nlabeled}~(PU) learning.  Fei \&~Liu~\cite{Fei:2015} took an alternate approach by ignoring any unlabeled data and learned a traditional supervised classifier using just the positive and biased-negative training examples.  Hsieh\etal~\cite{Hsieh:2018} combined the \underline{p}ositive, \underline{u}nlabeled, and \underline{b}iased \underline{n}egative~(PUbN) training sets into a single risk estimator that fits into the ERM~framework.

The primary contribution of this work is an empirical comparison of the PN, PU, and~PUbN estimators on 20~newsgroups~\cite{20newsgroups} classification under negative covariate shift.  The remainder of this document is structured as follows.  Section~\ref{sec:RiskEstimators} describes the PN, PU, and~PUbN risk estimation functions. Section~\ref{sec:Architectures} describes the two neural network architectures used in our experiments. Section~\ref{sec:20newsgroups} provides a brief overview of the 20~newsgroups dataset including how it is used in this paper. Section~\ref{sec:ExperimentalResults} describes our experimental setup, baselines, and results.  We provide brief concluding comments in Section~\ref{sec:Conclusions}.
\end{document}
