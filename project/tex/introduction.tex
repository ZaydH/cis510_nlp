\section{Introduction}\label{sec:Introduction}

Consider standard, supervised binary classification that learns a prediction function from independent random variable ${\X \in \domain}$ to dependent random variable~${\y \in \labels}$. The training set ${\train = \set{(\X_i,\y_i)}_{i=1}^{\size} \sim \joint(\X,\y)}$ is composed of positive- (${\y=\pCls}$) and negative-labeled (${\y=\nCls}$) examples sampled i.i.d.\ from unknown, joint distribution~$\joint$.

Collecting labeled data is often difficult and/or expensive.  In such cases, $\train$~may be too small to be adequately representative of~$\joint$.  \textit{Semi-supervised} learning supplements~$\train$ with an unlabeled set ${\unlabel = \set{x_i}_{i=1}^{\size_{\textnormal{U}}} \sim \marginal(\X)}$, where the geometry of marginal distribution~$\marginal$ serves as a regularizer on the learner.

\textit{Positive\-/unlabeled} (PU)~learning is a subclass of semi-supervised learning where it is prohibitively challenging to collect any data for one (i.e.,~the negative) class.  Therefore ${\train = \pos = \set{(\X_i,\pCls)}_{i=1}^{\size_{\textnormal{P}}} \sim \joint(\X,\pCls)}$.  PU~learning can be framed as both a transductive problem, where the goal is to correctly classify the elements of~$\unlabel$ as well as inductive where the model must generalize to an unseen test set. Real-world applications where positive-unlabeled learning has been applied include: disease-gene identification~\cite{Yang:2012}, land-cover classification~\cite{Li:2011}, matrix completion~\cite{Hsieh:2015}, protein similarity prediction~\cite{Elkan:2008}, outlier detection~\cite{Hido:2008,Scott:2009}, and time-series data~\cite{Nguyen:2011}.

PU~learning's limited labeled data means highly expressive learners, such as deep neural networks, can easily overfit.  Recent work~\cite{duPlessis:2014,Kiryo:2017} has attempted to address this overfitting with some success.

The remainder of this paper is structured as follows.  Section~\ref{sec:RelatedWork} reviews existing PU~methods with a focus on deep learning approaches.  Section~\ref{sec:Siamese} introduces .  In Section~\ref{sec:ddPU} we propose our new deep PU~learner, describing both the custom training scheme and novel losses functions it uses.  Experimental results are discussed in Section~\ref{sec:ExperimentalResults}. Section~\ref{sec:Conclusions} includes brief concluding remarks.
