\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry}

\input{global_macros}
\input{macros}

% Enable (uncolored) cross-reference hyperlinks
% Should always be last package loaded.
% See: https://tex.stackexchange.com/questions/103123/links-do-not-lead-to-right-pages
\usepackage[colorlinks=false]{hyperref}
\begin{document}
  \begin{center}
    {\Large \textbf{Text Classification under Negative Covariate Shift}}

    \vspace{6pt}{\large Project Proposal}

    \vspace{6pt}Zayd Hammoudeh
  \end{center}

  Consider the task of binary (supervised) classification.  Each sample is a tuple of feature vector~${\X \in \domain}$ and label~${\y \in \lbls}$.  Samples are generated from unknown joint probability distribution~$\joint$.  Given a hypothesis class~${\hypoCls=\setbuild{\dec}{\func{\dec}{\domain}{\real}}}$ and loss function~$\func{\loss}{\real \times \lbls}{\real}$, the ideal classifier is:

  \begin{equation}\label{eq:IdealClassifier}
    \dec\opt = \argmin_{\dec \in \hypoCls} \mathbb{E}_{(\X, \y) \sim \joint} \sbrack{\fLoss{\fDec{\X}, \y}}\text{.}
  \end{equation}

  Since $\joint$ is unknown, Eq.~\eqref{eq:IdealClassifier} cannot be determined exactly.  \textit{Empirical Risk Minimization}, defined in Eq.~\eqref{eq:ERM}, is used in practice for selection of the optimal learner,~${\dec\opt}$. Observe that the true expected loss is replaced with an empirical estimate using i.i.d.\ training set~${\train \sim \joint}$.

  \begin{equation}\label{eq:ERM}
    \dec\opt = \argmin_{\dec \in \hypoCls} \sum_{(\X_i,\y_i) \in \train} \fLoss{\fDec{\X_i}, \y_i}
  \end{equation}

  There are many real-world applications where $\train$ is not an i.i.d.\ sample. For example, given a huge (unlabeled) news corpus, a particular user may only be interested in a single topic,~$\interest$.  There are numerous irrelevant news topics,~${\irrel_1,\ldots,\irrel_n}$ where~$n$ is huge.  This user could label a few relevant articles she manually found as positive~($\pCls$). She could also label any irrelevant articles she encountered negative. Such an ad-hoc labeling mechanism generally induces \textit{covariate shift} (i.e.,~train and test marginal distributions) for the negative class. For instance, the user may fail to label documents from each negative topic~$\irrel_i$. Similarly, the training set's negative topic prior probabilities will not match the corresponding true priors. Constructing a classifier given negative covariate shift is know as \textit{positive, unlabeled, biased negative} (PUbN)~\textit{learning}.

  There are three common approaches to PUbN learning:

  \begin{enumerate}
    \item Supervised (PN -- Positive-Negative) Learning: Ignores the bias in the negative training examples.  Serves as a baseline of comparison and quantifies the effect of any correction.

    \item Positive\-/Unlabeled (PU) Learning: Premised on the idea that using the biased negative training examples can be inherently deleterious.  It constructs a binary classifier strictly using the positive examples and the residual unlabeled corpus.  An example where this approach was used is~\cite{Li:2010}.

    \item Positive, Unlabeled, biased Negative (PUbN) Learning: Constructs a classifier using some \textit{a~priori} knowledge of the negative training set's bias.  Similar to~\cite{Hsieh:2019}, this project assumes knowledge of the negative training sets prior probabilities.
  \end{enumerate}

  \paragraph{Goal} This project will explore negative covariate shift's effect on news topic classification.  The experiments in this section are inspired by Fei \&~Liu~\cite{Fei:2015} and Hsieh\etal~\cite{Hsieh:2019}.

  \paragraph{Experimental Setup} Using the 20~newsgroups dataset, one topic will be selected as the positive class and all remaining~(19) topics will form joint form the negative class.  The experiments will be divided into three categories:

  \begin{itemize}
    \item \textit{In-Training}: All negative subclasses (${\irrel_i,\ldots,\irrel_n}$) are present in both the train and test sets.  The only difference is their train/test marginal distributions.

    \item \textit{Out-of-Training}: Some negative subclasses are only present in the test set. Beside that difference, the train and test marginal distributions are identical.

    \item \textit{Combined}: Synthesis of in-training and out-of-training.  Bias includes missing train set subclasses as well as marginal distribution shifts.  Most challenging case.
  \end{itemize}

  \noindent
  This procedure will be repeated with different topics as the positive class to ensure as representative of an experiment set as possible.

  \paragraph{Implementation Plan} The PN, PU, and PUbN implementations will share the same standard neural network architecture implemented in PyTorch.  There is an embedding of some kind (at minimum word but potentially also character as well), a bidirectional (possibly stacked) RNN, and an output feed forward. Where the architectures differ is in the risk estimator used for gradient descent.  PN~will use standard logistic loss.  PU~will use the nnPU risk estimator by Kiryo\etal\ in~\cite{Kiryo:2017}.  PUbN will use~\cite{Hsieh:2019}'s risk estimator.

  \bibliographystyle{ieeetr}
  \bibliography{bib/ref.bib}
\end{document}
